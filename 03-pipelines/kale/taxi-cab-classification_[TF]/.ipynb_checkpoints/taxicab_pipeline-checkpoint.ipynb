{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxi Cab Classification (prior to TF2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook presents a simplified version of Kubeflow's *taxi cab clasification* pipeline, built upon TFX components.\n",
    "\n",
    "Here all the pipeline components are stripped down to their core to showcase how to run it in a self-contained local Juyter Noteobok.\n",
    "\n",
    "Additionally, the pipeline has been upgraded to work with Python3 and all major libraries (Tensorflow, Tensorflow Transform, Tensorflow Model Analysis, Tensorflow Data Validation, Apache Beam) have been bumped to their latests versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15.0 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install apache_beam tensorflow_transform tensorflow_model_analysis tensorflow_data_validation --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have to restart the workbook after installing these packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "import apache_beam as beam\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from apache_beam.io import textio\n",
    "from apache_beam.io import tfrecordio\n",
    "\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.coders.csv_coder import CsvCoder\n",
    "from tensorflow_transform.coders.example_proto_coder import ExampleProtoCoder\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import metadata_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/'\n",
    "TRAIN_DATA = os.path.join(DATA_DIR, 'taxi-cab-classification/train.csv')\n",
    "EVALUATION_DATA = os.path.join(DATA_DIR, 'taxi-cab-classification/eval.csv')\n",
    "\n",
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 12]\n",
    "CATEGORICAL_FEATURE_KEYS = ['trip_start_hour', 'trip_start_day', 'trip_start_month']\n",
    "\n",
    "DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "BUCKET_FEATURE_KEYS = ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "OOV_SIZE = 10\n",
    "\n",
    "VOCAB_FEATURE_KEYS = ['pickup_census_tract', 'dropoff_census_tract', 'payment_type', 'company',\n",
    "    'pickup_community_area', 'dropoff_community_area']\n",
    "\n",
    "# allow nan values in these features.\n",
    "OPTIONAL_FEATURES = ['dropoff_latitude', 'dropoff_longitude', 'pickup_census_tract', 'dropoff_census_tract',\n",
    "    'company', 'trip_seconds', 'dropoff_community_area']\n",
    "\n",
    "LABEL_KEY = 'tips'\n",
    "FARE_KEY = 'fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pipeline-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "EPOCHS = 1\n",
    "STEPS = 3\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_LAYER_SIZE = '1500'\n",
    "LEARNING_RATE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "functions"
    ]
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "# tf.get_logger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of the TFDV functions: https://www.tensorflow.org/tfx/tutorials/data_validation/chicago_taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:data_validation"
    ]
   },
   "outputs": [],
   "source": [
    "vldn_output = os.path.join(DATA_DIR, 'validation')\n",
    "\n",
    "# TODO: Understand why this was used in the conversion to the output json\n",
    "# key columns: list of the names for columns that should be treated as unique keys.\n",
    "key_columns = ['trip_start_timestamp']\n",
    "\n",
    "# read the first line of the cvs to have and ordered list of column names \n",
    "# (the Schema will scrable the features)\n",
    "with open(TRAIN_DATA) as f:\n",
    "    column_names = f.readline().strip().split(',')\n",
    "\n",
    "stats = tfdv.generate_statistics_from_csv(data_location=TRAIN_DATA)\n",
    "schema = tfdv.infer_schema(stats)\n",
    "\n",
    "eval_stats = tfdv.generate_statistics_from_csv(data_location=EVALUATION_DATA)\n",
    "anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "\n",
    "# Log anomalies\n",
    "for feature_name, anomaly_info in anomalies.anomaly_info.items():\n",
    "    logging.getLogger().error(\n",
    "        'Anomaly in feature \"{}\": {}'.format(\n",
    "            feature_name, anomaly_info.description))\n",
    "    \n",
    "# show inferred schema\n",
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resolve anomalies\n",
    "company = tfdv.get_feature(schema, 'company')\n",
    "company.distribution_constraints.min_domain_mass = 0.9\n",
    "\n",
    "# Add new value to the domain of feature payment_type.\n",
    "payment_type_domain = tfdv.get_domain(schema, 'payment_type')\n",
    "payment_type_domain.value.append('Prcard')\n",
    "\n",
    "# Validate eval stats after updating the schema \n",
    "updated_anomalies = tfdv.validate_statistics(eval_stats, schema)\n",
    "tfdv.display_anomalies(updated_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an overview of the TFT functions: https://www.tensorflow.org/tfx/tutorials/transform/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:data_transformation",
     "prev:data_validation"
    ]
   },
   "outputs": [],
   "source": [
    "def to_dense(tensor):\n",
    "    \"\"\"Takes as input a SparseTensor and return a Tensor with correct default value\n",
    "    Args:\n",
    "      tensor: tf.SparseTensor\n",
    "    Returns:\n",
    "      tf.Tensor with default value\n",
    "    \"\"\"\n",
    "    if not isinstance(tensor, tf.sparse.SparseTensor):\n",
    "        return tensor\n",
    "    if tensor.dtype == tf.string:\n",
    "        default_value = ''\n",
    "    elif tensor.dtype == tf.float32:\n",
    "        default_value = 0.0\n",
    "    elif tensor.dtype == tf.int32:\n",
    "        default_value = 0\n",
    "    else:\n",
    "        raise ValueError(f\"Tensor type not recognized: {tensor.dtype}\")\n",
    "\n",
    "    return tf.squeeze(tf.sparse_to_dense(tensor.indices,\n",
    "                               [tensor.dense_shape[0], 1],\n",
    "                               tensor.values, default_value=default_value), axis=1)\n",
    "    # TODO: Update to below version\n",
    "    # return tf.squeeze(tf.sparse.to_dense(tensor, default_value=default_value), axis=1)\n",
    "\n",
    "\n",
    "def preprocess_fn(inputs):\n",
    "    \"\"\"tf.transform's callback function for preprocessing inputs.\n",
    "    Args:\n",
    "      inputs: map from feature keys to raw not-yet-transformed features.\n",
    "    Returns:\n",
    "      Map from string feature key to transformed feature operations.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    for key in DENSE_FLOAT_FEATURE_KEYS:\n",
    "        # Preserve this feature as a dense float, setting nan's to the mean.\n",
    "        outputs[key] = tft.scale_to_z_score(to_dense(inputs[key]))\n",
    "\n",
    "    for key in VOCAB_FEATURE_KEYS:\n",
    "        # Build a vocabulary for this feature.\n",
    "        if inputs[key].dtype == tf.string:\n",
    "            vocab_tensor = to_dense(inputs[key])\n",
    "        else:\n",
    "            vocab_tensor = tf.as_string(to_dense(inputs[key]))\n",
    "        outputs[key] = tft.compute_and_apply_vocabulary(\n",
    "            vocab_tensor, vocab_filename='vocab_' + key,\n",
    "            top_k=VOCAB_SIZE, num_oov_buckets=OOV_SIZE)\n",
    "\n",
    "    for key in BUCKET_FEATURE_KEYS:\n",
    "        outputs[key] = tft.bucketize(to_dense(inputs[key]), FEATURE_BUCKET_COUNT)\n",
    "\n",
    "    for key in CATEGORICAL_FEATURE_KEYS:\n",
    "        outputs[key] = tf.cast(to_dense(inputs[key]), tf.int64)\n",
    "\n",
    "    taxi_fare = to_dense(inputs[FARE_KEY])\n",
    "    taxi_tip = to_dense(inputs[LABEL_KEY])\n",
    "    # Test if the tip was > 20% of the fare.\n",
    "    tip_threshold = tf.multiply(taxi_fare, tf.constant(0.2))\n",
    "    outputs[LABEL_KEY] = tf.logical_and(\n",
    "        tf.logical_not(tf.math.is_nan(taxi_fare)),\n",
    "        tf.greater(taxi_tip, tip_threshold))\n",
    "\n",
    "    for key in outputs:\n",
    "        if outputs[key].dtype == tf.bool:\n",
    "            outputs[key] = tft.compute_and_apply_vocabulary(tf.as_string(outputs[key]),\n",
    "                                             vocab_filename='vocab_' + key)\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trns_output = os.path.join(DATA_DIR, \"transformed\")\n",
    "if os.path.exists(trns_output):\n",
    "    shutil.rmtree(trns_output)\n",
    "\n",
    "tft_input_metadata = dataset_metadata.DatasetMetadata(schema)\n",
    "\n",
    "runner = 'DirectRunner'\n",
    "with beam.Pipeline(runner, options=None) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(trns_output, 'tmp')):\n",
    "        converter = CsvCoder(column_names, tft_input_metadata.schema)\n",
    "\n",
    "        # READ TRAIN DATA\n",
    "        train_data = (\n",
    "                p\n",
    "                | 'ReadTrainData' >> textio.ReadFromText(TRAIN_DATA, skip_header_lines=1)\n",
    "                | 'DecodeTrainData' >> beam.Map(converter.decode))\n",
    "\n",
    "        # TRANSFORM TRAIN DATA (and get transform_fn function)\n",
    "        transformed_dataset, transform_fn = (\n",
    "                (train_data, tft_input_metadata) | beam_impl.AnalyzeAndTransformDataset(preprocess_fn))\n",
    "        transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "        # SAVE TRANSFORMED TRAIN DATA\n",
    "        _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(trns_output, 'train'),\n",
    "            coder=ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "        # READ EVAL DATA\n",
    "        eval_data = (\n",
    "                p\n",
    "                | 'ReadEvalData' >> textio.ReadFromText(EVALUATION_DATA, skip_header_lines=1)\n",
    "                | 'DecodeEvalData' >> beam.Map(converter.decode))\n",
    "\n",
    "        # TRANSFORM EVAL DATA (using previously created transform_fn function)\n",
    "        eval_dataset = (eval_data, tft_input_metadata)\n",
    "        transformed_eval_data, transformed_metadata = (\n",
    "            (eval_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "\n",
    "        # SAVE EVAL DATA\n",
    "        _ = transformed_eval_data | 'WriteEvalData' >> tfrecordio.WriteToTFRecord(\n",
    "            os.path.join(trns_output, 'eval'),\n",
    "            coder=ExampleProtoCoder(transformed_metadata.schema))\n",
    "\n",
    "        # SAVE transform_fn FUNCTION FOR LATER USE\n",
    "        # TODO: check out what is the transform function (transform_fn) that came from previous step\n",
    "        _ = (transform_fn | 'WriteTransformFn' >> transform_fn_io.WriteTransformFn(trns_output))\n",
    "\n",
    "        # SAVE TRANSFORMED METADATA\n",
    "        metadata_io.write_metadata(\n",
    "            metadata=tft_input_metadata,\n",
    "            path=os.path.join(trns_output, 'metadata'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimator API: https://www.tensorflow.org/guide/premade_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:train",
     "prev:data_transformation"
    ]
   },
   "outputs": [],
   "source": [
    "def training_input_fn(transformed_output, transformed_examples, batch_size, target_name):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      transformed_output: tft.TFTransformOutput\n",
    "      transformed_examples: Base filename of examples\n",
    "      batch_size: Batch size.\n",
    "      target_name: name of the target column.\n",
    "    Returns:\n",
    "      The input function for training or eval.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
    "        file_pattern=transformed_examples,\n",
    "        batch_size=batch_size,\n",
    "        features=transformed_output.transformed_feature_spec(),\n",
    "        reader=tf.data.TFRecordDataset,\n",
    "        shuffle=True)\n",
    "    transformed_features = dataset.make_one_shot_iterator().get_next()\n",
    "    transformed_labels = transformed_features.pop(target_name)\n",
    "    return transformed_features, transformed_labels\n",
    "\n",
    "\n",
    "def get_feature_columns():\n",
    "    \"\"\"Callback that returns a list of feature columns for building a tf.estimator.\n",
    "    Returns:\n",
    "      A list of tf.feature_column.\n",
    "    \"\"\"\n",
    "    return (\n",
    "            [tf.feature_column.numeric_column(key, shape=()) for key in DENSE_FLOAT_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=VOCAB_SIZE + OOV_SIZE)) for key in VOCAB_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=FEATURE_BUCKET_COUNT, default_value=0)) for key in BUCKET_FEATURE_KEYS] +\n",
    "            [tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(key, num_buckets=num_buckets, default_value=0)) for key, num_buckets in zip(CATEGORICAL_FEATURE_KEYS, MAX_CATEGORICAL_FEATURE_VALUES)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_output = os.path.join(DATA_DIR, \"training\")\n",
    "if os.path.exists(training_output):\n",
    "    shutil.rmtree(training_output)\n",
    "\n",
    "hidden_layer_size = [int(x.strip()) for x in HIDDEN_LAYER_SIZE.split(',')]\n",
    "\n",
    "tf_transform_output = tft.TFTransformOutput(trns_output)\n",
    "\n",
    "# Set how often to run checkpointing in terms of steps.\n",
    "config = tf.estimator.RunConfig(save_checkpoints_steps=1000)\n",
    "n_classes = tf_transform_output.vocabulary_size_by_name(\"vocab_\" + LABEL_KEY)\n",
    "# Create estimator\n",
    "estimator =  tf.estimator.DNNClassifier(\n",
    "                feature_columns=get_feature_columns(),\n",
    "                hidden_units=hidden_layer_size,\n",
    "                n_classes=n_classes,\n",
    "                config=config,\n",
    "                model_dir=training_output)\n",
    "\n",
    "# TODO: Simplify all this: https://www.tensorflow.org/guide/premade_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn=lambda: training_input_fn(\n",
    "                                    tf_transform_output, \n",
    "                                    os.path.join(trns_output, 'train' + '*'),\n",
    "                                    BATCH_SIZE, \n",
    "                                    \"tips\"), \n",
    "                steps=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "block:eval",
     "prev:train"
    ]
   },
   "outputs": [],
   "source": [
    "eval_result = estimator.evaluate(input_fn=lambda: training_input_fn(\n",
    "                                                    tf_transform_output, \n",
    "                                                    os.path.join(trns_output, 'eval' + '*'),\n",
    "                                                    BATCH_SIZE, \n",
    "                                                    \"tips\"), \n",
    "                                 steps=50)\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF Model Analysis docs: https://www.tensorflow.org/tfx/model_analysis/get_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": [
     "skip"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Implement model load and params analysis\n",
    "\n",
    "def eval_input_receiver_fn(transformed_output):\n",
    "    \"\"\"Build everything needed for the tf-model-analysis to run the model.\n",
    "    Args:\n",
    "      transformed_output: tft.TFTransformOutput\n",
    "    Returns:\n",
    "      EvalInputReceiver function, which contains:\n",
    "        - Tensorflow graph which parses raw untranformed features, applies the\n",
    "          tf-transform preprocessing operators.\n",
    "        - Set of raw, untransformed features.\n",
    "        - Label against which predictions will be compared.\n",
    "    \"\"\"\n",
    "    serialized_tf_example = tf.compat.v1.placeholder(\n",
    "        dtype=tf.string, shape=[None], name='input_example_tensor')\n",
    "    features = tf.io.parse_example(serialized_tf_example, transformed_output.raw_feature_spec())\n",
    "    transformed_features = transformed_output.transform_raw_features(features)\n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=transformed_features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=transformed_features[LABEL_KEY])\n",
    "\n",
    "# EXPORT MODEL\n",
    "eval_model_dir = os.path.join(training_output, 'tfma_eval_model_dir')\n",
    "tfma.export.export_eval_savedmodel(\n",
    "    estimator=estimator,\n",
    "    export_dir_base=eval_model_dir,\n",
    "    eval_input_receiver_fn=(lambda: eval_input_receiver_fn(tf_transform_output)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kubeflow_notebook": {
   "docker_image": "docker.io/stefanofioravanzo/kale-notebook:0.9",
   "experiment": {
    "id": "new",
    "name": "Taxicab"
   },
   "experiment_name": "Taxicab",
   "pipeline_description": "Use TFX components and Apache Beam to run a ML job over the Chicago taxicab dataset",
   "pipeline_name": "taxicab",
   "volumes": [
    {
     "annotations": [],
     "mount_point": "/home/jovyan",
     "name": "workspace-kale-dt49raygc",
     "size": 6,
     "size_type": "Gi",
     "snapshot": false,
     "type": "clone"
    }
   ]
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
