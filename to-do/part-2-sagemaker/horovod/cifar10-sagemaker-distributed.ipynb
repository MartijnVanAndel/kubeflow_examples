{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Amazon SageMaker\n",
    "\n",
    "In this notebook we use the SageMaker Python SDK to setup and run a distributed training job.\n",
    "SageMaker makes it easy to train models across a cluster containing a large number of machines, without having to explicitly manage those resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import essentials packages, start a sagemaker session and specify the bucket name you created in the pre-requsites section of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket_name = '<your-globally-unique-bucket-name>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Specify hyperparameters, instance type and number of instances to distribute training to. The `hvd_processes_per_host` corresponds to the number of GPUs per instances. \n",
    "For example, if you choose:\n",
    "```\n",
    "hvd_instance_type = 'ml.p3.2xlarge'\n",
    "hvd_instance_count = 2\n",
    "hvd_processes_per_host = 1\n",
    "```\n",
    "\n",
    "This is spread across 2 instances (or nodes). SageMaker automatically takes care of spinning up these instances and making sure they can communiate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 5, \n",
    "                   'learning-rate': 0.001,\n",
    "                   'momentum': 0.9,\n",
    "                   'weight-decay': 2e-4,\n",
    "                   'optimizer': 'adam',\n",
    "                   'batch-size' : 256}\n",
    "\n",
    "hvd_instance_type = 'ml.p3.2xlarge'\n",
    "hvd_instance_count = 2\n",
    "hvd_processes_per_host = 1\n",
    "\n",
    "print('Distributed training with a total of {} workers'.format(hvd_processes_per_host*hvd_instance_count))\n",
    "print('{} x {} instances with {} processes per instance'.format(hvd_instance_count, hvd_instance_type, hvd_processes_per_host))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** In this cell we create a SageMaker estimator, by providing it with all the information it needs to launch instances and execute training on those instances.\n",
    "\n",
    "Since we're using horovod for distributed training, we specify `distributions` to mpi which is used by horovod.\n",
    "\n",
    "In the TensorFlow estimator call, we specify training script under `entry_point` and dependencies under `code`. SageMaker automatically copies these files into a TensorFlow container behind the scenes, and are executed on the training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "output_path = 's3://{}/'.format(bucket_name)\n",
    "job_name = 'sm-dist-{}x{}-workers-'.format(hvd_instance_count, hvd_processes_per_host) + time.strftime('%Y-%m-%d-%H-%M-%S-%j', time.gmtime())\n",
    "model_dir = output_path + 'tensorboard_logs/' + job_name\n",
    "\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': hvd_processes_per_host,\n",
    "                    'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO -x OMPI_MCA_btl_vader_single_copy_mechanism=none'\n",
    "                        }\n",
    "                }\n",
    "\n",
    "estimator_hvd = TensorFlow(base_job_name='hvd-cifar10-tf',\n",
    "                       source_dir='code',\n",
    "                       entry_point='cifar10-multi-gpu-horovod-sagemaker.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.14',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=hvd_instance_count, \n",
    "                       train_instance_type=hvd_instance_type,\n",
    "                       output_path=output_path,\n",
    "                       model_dir=model_dir,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n",
    "                       metric_definitions=[{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}],\n",
    "                       distributions=distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Specify dataset locations in Amazon S3 and then call the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 's3://{}/cifar10-dataset/train'.format(bucket_name)\n",
    "val_path = 's3://{}/cifar10-dataset/validation'.format(bucket_name)\n",
    "eval_path = 's3://{}/cifar10-dataset/eval/'.format(bucket_name)\n",
    "\n",
    "estimator_hvd.fit({'train': train_path,'validation': val_path,'eval': eval_path}, \n",
    "                  job_name=job_name, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_hvd.attach(training_job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in the `estimator_hvd.fit()` function above, change`wait=True` if you want to see the training output in the Jupyter notebook.\n",
    "Advantage of setting `wait=False`, is that you can continue to run cells. \n",
    "Since we're unblocked due to `wait=False` we can now launch tensorboard in the notebook and monitor progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
